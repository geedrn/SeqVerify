#!/usr/bin/env python
 
import argparse
import os
import sys
import configparser
import shutil
from seqver_functions import *
from seqver_plots import *
from seqver_genomeupdate import *
from seqver_lofFinder import *
from seqver_gtf import *
import logging

parser = argparse.ArgumentParser()

#Parses commandline arguments:
#Main arguments
parser.add_argument('--output', type=str, required=False, default="output") #Name of the output folder
parser.add_argument('--reads_1', type=str, required=False) #Name/Path of First .fq file for reads
parser.add_argument('--reads_2', type=str, required=False) #Name/Path of Second .fq file for reads
parser.add_argument('--untargeted', type=str, required=False, nargs='+') #Name/Path of transgenes to be appended to the genome as extra sequences
parser.add_argument('--targeted', type=str, required=False) #Name/Path of command file instructing where to insert known-insertion transgenes
parser.add_argument('--keepgoing',action='store_const',const=True,required=False,default=False) #continue to next part after completing
parser.add_argument('--keep_temp', action='store_const',const=True, required=False, default=False) #HIGHLY RECOMMENDED - Option to delete temporary files when the pipeline is done

#Genome arguments
parser.add_argument('--genome', type=str, required=False, default=f"{os.getcwd()}/seqverify_defaults/chm13v2.0.fa") #Name/Path of genome file (CHM13 recommended) used to perform everything but variant calling.
parser.add_argument('--gtf', type=str, required=False, default=f"{os.getcwd()}/seqverify_defaults/chm13v2.0_RefSeq_Liftoff_v5.1.gff3") #Name/Path of gtf file for editing

#KRAKEN2 arguments
parser.add_argument('--kraken', action='store_const',const=True, required=False,default=False) #Flag enabling KRAKEN2/BRACKEN analysis
parser.add_argument('--database',type=str,required=False, default=f"{os.getcwd()}/seqverify_defaults/seqverify_database") #Path to KRAKEN2 database

#Insertion Site Detection Arguments
parser.add_argument('--granularity',type=int, required=False, default=500) #How far apart two insertions can be to count as the same insertion site
parser.add_argument('--threads',type=int,required=False, default=1) #How many threads seqverify can use
parser.add_argument('--max_mem', type=str, required=False, default='16G') #Max memory allowed, ending in capital "G" or "M"
parser.add_argument('--min_matches', type=int, required=False,default=1) #How many times an insertion needs to align to the reference genome to show up in the readout
parser.add_argument('--start', type=str, required=False, default="all") #which section of the code to run
parser.add_argument('--mitochondrial',action='store_const',const=True,required=False,default=False) #runs insertion site detection on mitochondrial DNA
parser.add_argument('--stringency',type=float, required=False, default=0.005) #sets stringency for the confidence score calculations
parser.add_argument('--spurious_filtering_threshold', type=float, required=False, default=0.00001)

#CNV/Plotting Arguments
parser.add_argument('--manual_plots',action='store_const',const=True,required=False,default=False) #Flag enabling matplotlib plots instead of IGV plots 
parser.add_argument('--bin_size',type=int, required=False, default=100000) #If manual plots are enabled, the size of each bin in the histogram

#Variant Calling Arguments
parser.add_argument('--variant_calling',nargs=2,type=str,required=False) #Space-separated: the Path/File to the hg38 genome, the SnpEff.jar config path, the clinvar DB path
parser.add_argument('--variant_intensity',type=str,required=False,default="MODERATE") #Minimum severity needed for a variant to show up in the readout, can be set to "MODIFIER", "LOW", "MODERATE", or "HIGH"
parser.add_argument('--variant_window_size',type=int,required=False,default=10000) #Makes sure all variants within variant_window_size of any command's start are printed, regardless of their quality or intensity.  

#Other
parser.add_argument('--download_defaults',action='store_const',const=True,required=False,default=False) #Downloads hg38.p14, CHM13 V2.0, and the PFPlus-8GB KRAKEN2 DB for ease of use. Terminates the program after doing that
parser.add_argument('--similarity',nargs=2,type=str,required=False) #Runs the SNP similarity checker
parser.add_argument('--min_quality',type=int,required=False,default=3)
parser.add_argument('--config',type=str,required=False,default=None)

args = parser.parse_args()

if args.config is not None:
    config = configparser.ConfigParser(allow_no_value=True)
    config.read(args.config)

    args.output = config["MAIN"]["output"]
    args.reads_1 = config["MAIN"]["reads_1"]
    args.reads_2 = config["MAIN"]["reads_2"]
    args.untargeted = config["MAIN"]["untargeted"].split(" ")
    args.targeted = config["MAIN"]["targeted"]
    args.keepgoing = config["MAIN"]["keepgoing"]
    args.keep_temp = bool(config["MAIN"]["keep_temp"])
    args.genome = config["GENOME"]["genome"]
    args.gtf = config["GENOME"]["gtf"]
    args.kraken = bool(config["KRAKEN2"]["kraken"])
    args.database = config["KRAKEN2"]["database"]
    args.granularity = int(config["INSERTION"]["granularity"])
    args.threads = int(config["INSERTION"]["threads"])
    args.max_mem = config["INSERTION"]["max_mem"]
    args.min_matches = int(config["INSERTION"]["min_matches"])
    args.start = config["INSERTION"]["start"]
    args.mitochondrial = config["INSERTION"]["mitochondrial"]
    args.stringency = float(config["INSERTION"]["stringency"])
    args.spurious_filtering_threshold = float(config["INSERTION"]["spurious_filtering_threshold"])
    args.manual_plots = config["CNV"]["manual_plots"]
    args.bin_size = int(config["CNV"]["bin_size"])
    args.variant_calling = eval(config["VARIANT"]["variant_calling"])
    args.variant_intensity = config["VARIANT"]["variant_intensity"]
    args.variant_window_size = int(config["VARIANT"]["variant_window_size"])
    args.min_quality = int(config["OTHER"]["min_quality"])
output_name = args.output 

#Stores Variable/Folder Names:
folder = f"{output_name}_seqverify"
temp_folder =f"{output_name}_seqverify_temp"
marker_list_file = f"seqverify_{output_name}_transgene_list.fa"
genome_with_markers = f"seqverify_{output_name}_genome_with_markers.fa"
aligned_with_markers = f"seqverify_{output_name}_markers.sam"
aligned_diff_chr = f"seqverify_{output_name}_markers_diff_chr.sam"
aligned_diff_chr_bam = f"seqverify_{output_name}_markers_diff_chr.bam"
header = f"seqverify_{output_name}_markers_header.sam"
overall_coverage = f"seqverify_{output_name}_markers_coverage.cov"
pytor = f"{output_name}.pytor"
pytor_gc = f"{output_name}_gc.pytor"
pytor_conf = f"{output_name}_genome_conf.py"
unaligned = f"seqverify_{output_name}_unaligned.sam"
unaligned_R1 = f"seqverify_{output_name}_unaligned_R1.fastq"
unaligned_R2 = f"seqverify_{output_name}_unaligned_R2.fastq"
variant_genome_aligned = f"seqverify_{output_name}_variant.sam"
variant_genome_aligned_bam = f"seqverify_{output_name}_variant.bam"
variant_vcf = f"seqverify_{output_name}.vcf"
variant_vcf_ann = f"seqverify_{output_name}.ann.vcf"
variant_lof = f"seqverify_{output_name}_variants.tsv"
bed_file = f"{output_name}.bed"
vcf_similarity = f"seqverify_{output_name}_bcfstats.txt"
vcf_isec = f"seqverify_{output_name}_unique.vcf"
new_gtf_unsorted = f"seqverify_{output_name}_unsorted.gtf"
new_gtf_sorted = f"seqverify_{output_name}.gtf"

#Output Subdirectories:
folder_insertion = f"{folder}/insertion"
folder_cnv = f"{folder}/copy_number"
folder_kraken = f"{folder}/kraken"
folder_snp =f"{folder}/variant_calling"   

# File name with output
def setup_logging(folder, output):
    try:
        # Ensure the log directory exists
        os.makedirs(os.path.dirname(f"{folder}/{output}.log"), exist_ok=True)
        
        log_file = f"{folder}/{output}.log"
        logging.basicConfig(
            filename=log_file,
            level=logging.DEBUG,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        print(f"Logging initialized. Log file: {log_file}")
        logging.info(f"Logging initialized. Log file: {log_file}")
    except Exception as e:
        print(f"Error setting up logging: {str(e)}")
        raise

setup_logging(folder, args.output)
print("SeqVerify execution starts")
logging.info("SeqVerify execution starts")

if not run_command(f'mkdir -p {temp_folder}'):
    print(f"Error: Failed to create folder {temp_folder}")

if args.similarity is not None: #similarity also requires --output to be set, takes two arguments: first the original cell line, then the modified cell line, then --min_quality can be set to change the minimum quality (default: 100).
    print("Starting similarity check")
    logging.info("Starting similarity check")
    compare(args.similarity[0],args.similarity[1],args.min_quality,temp_folder,folder,vcf_similarity,vcf_isec)
    print("Similarity check completed")
    logging.info("Similarity check completed")
    sys.exit(0)

#The pathFinder function detects whether arguments are paths or names and acts accordingly such that it can handle both cases for ease of use.
reads_1_path = pathFinder(args.reads_1)
reads_2_path = pathFinder(args.reads_2)
genome_source_path = pathFinder(args.genome)

marker_sources_path = None
exact_path = None
database = None
gtf_file = None
gtf = None  

if args.download_defaults: #If --download_defaults is set, downloads the three databases and quits the program
    #Note: These FTP links are likely to change over time and break the command. 
    #Note: Please open an issue on GitHub if they do! https://github.com/mpiersonsmela/SeqVerify
    #Note: If download_defaults is set, the system stops after downloading the default files
    print("Downloading default files")
    print("Making seqverify_defaults directory")
    run_command('mkdir seqverify_defaults')

    #Downloads chm13v2.0 as a reference genome for everything except the variant calling
    print("Downloading T2T CHM13 fasta file")
    run_command('curl -OJX GET "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/CHM13/assemblies/analysis_set/chm13v2.0.fa.gz" --output-dir seqverify_defaults')
    run_command('gunzip seqverify_defaults/chm13v2.0.fa.gz')

    #Downloads chm13v2.0's gff3 file for gtf editing
    print("Downloading T2T CHM13 gff3 file")
    run_command('curl -OJX GET "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/CHM13/assemblies/annotation/chm13v2.0_RefSeq_Liftoff_v5.1.gff3.gz" --output-dir seqverify_defaults')
    run_command('gunzip seqverify_defaults/chm13v2.0_RefSeq_Liftoff_v5.1.gff3.gz')

    #Downloads GRCh38 version 105 primary assembly (no alt contigs) as a reference genome for the variant calling
    print("Downloading hg38 assembly fasta file")
    run_command('curl -OJX GET "https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz" --output-dir seqverify_defaults')
    run_command('gunzip seqverify_defaults/hg38.fa.gz')

    #Downloads a KRAKEN-valid 8GB database, PlusPF_8GB
    print("Making seqverify_defaults/seqverify_database")
    run_command('mkdir seqverify_defaults/seqverify_database')
    
    print("Getting kraken database")
    run_command('curl -OJX GET "https://genome-idx.s3.amazonaws.com/kraken/k2_pluspf_08gb_20230605.tar.gz" --output-dir seqverify_defaults')
    run_command('tar -xzf seqverify_defaults/k2_pluspf_08gb_20230605.tar.gz -C seqverify_defaults/seqverify_database')

    #Downloads the snpEff config file for ease of access during variant calling
    print("Downloading snpEFF config file")
    run_command('curl -OJX GET "https://raw.githubusercontent.com/pcingola/SnpEff/master/config/snpEff.config" --output-dir seqverify_defaults')

    #Downloads the latest version of the ClinVar database for variant calling purposes
    print("Downloading clinvar vcf file")
    run_command('curl -OJX GET "https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz" --output-dir seqverify_defaults')
    run_command('curl -OJX GET "https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz.tbi" --output-dir seqverify_defaults')
    run_command('gunzip seqverify_defaults/clinvar.vcf.gz')
    
    print("Download complete")
    sys.exit(0)

granularity_threshold = args.granularity

if args.untargeted is not None:
    marker_sources_path = [pathFinder(i) for i in args.untargeted]

if args.targeted is not None:
    exact_path = pathFinder(args.targeted)

if args.database is not None:
    database = pathFinder(args.database)

if args.gtf is not None:
    gtf_file = pathFinder(args.gtf)

if exact_path is not None:
    commands = commandHandler(genome_source_path,exact_path,temp_folder,genome_with_markers,return_commands_only = False)

min_matches = args.min_matches
bin_size = args.bin_size
threads = args.threads

#Checks for a bin size that CNVPytor will recognize
if args.bin_size != 100000:
    if args.bin_size % 100 == 0:
        bin_size = args.bin_size
    else:
        raise ValueError("Custom bin size not divisible by 100")

#Deals with memory logic, converts max_mem argument into number of bytes of memory requested in order to build optimal BWA indexing block size
if args.max_mem[-1] == "M":
    max_mem = int(args.max_mem[:-1])*1000000
elif args.max_mem[-1] == "G":
    max_mem = int(args.max_mem[:-1])*1000000000
elif type(args.max_mem) == 'int':
    max_mem = int(args.max_mem)

base_block = max_mem / 8
#Set Java max mem for Java subprocesses (snpEff, etc)
os.environ['_JAVA_OPTIONS'] = '-Xmx'+args.max_mem
run_command('echo $_JAVA_OPTIONS')

logging.info(f"Number of threads: {threads}")
logging.info(f"Max memory: {max_mem / (1024 * 1024 * 1024):.2f} GB")

if args.start == "beginning" or args.start == "all":
    #Preliminary Work - Generates the output folders given a specified --output string as well as the temp folder for the pipeline to work within
    run_command(f'mkdir -p {folder_insertion}')

        
    #Automatic Preparation of Collated Genome:
    #Collates all markers/transgenes into a single FASTA file for ease of handling
    
    print("Merging transgenes into genome")
    logging.info("Automatically preparing collated genome")
    
    if args.gtf is not None:
        logging.info(f"gtf file specified: {gtf_file}")
        if check_file_exists(gtf_file):
            if exact_path is not None:
                logging.info("Exact path is specified. Proceeding with gtf editing.")
                print("gtf file specified. Proceeding with gtf editing.")
                try:
                    logging.info(f"Attempting to edit gtf file: {gtf_file}")
                    logging.info(f"Using commands from: {exact_path}")
                    logging.info(f"Temporary folder: {temp_folder}")
                    logging.info(f"Insertion folder: {folder_insertion}")
                    logging.info(f"Unsorted gtf output: {new_gtf_unsorted}")
                    logging.info(f"Sorted gtf output: {new_gtf_sorted}")
                    
                    success = gtfEdit(gtf_file, commands, temp_folder, folder_insertion, new_gtf_unsorted, new_gtf_sorted)
                    
                    if success:
                        logging.info("gtf editing completed successfully")
                        logging.info(f"Check for output file: {folder_insertion}/{new_gtf_sorted}")
                    else:
                        logging.info("gtf editing failed")
                except Exception as e:
                    logging.info(f"Error during gtf editing: {str(e)}")
                    logging.info("Proceeding without edited gtf file.")
                    print("Failed to edit gtf file. Proceeding without edited gtf file.")
            else:
                logging.info("Exact path is not specified (Targeted option is off). Copying gtf file without modifications.")
                copy_command = f"cp {gtf_file} {folder_insertion}/{new_gtf_sorted}"
                run_command(copy_command)
        else:
            pass
    else:
        print("No gtf file specified. Skipping gtf processing.")

    logging.info("gtf processing complete")
    print("Beginning genome index preparation")
    logging.info("Beginning genome index preparation")
    
    logging.info(f"Copying reference genome to: {temp_folder}/{genome_with_markers}")
    run_command(f'cp {genome_source_path} {temp_folder}/{genome_with_markers}')

    #Check if input files exist
    check_mandatory_files(f"{temp_folder}/{genome_with_markers}")
    check_mandatory_files(reads_1_path)
    check_mandatory_files(reads_2_path)
    
    if marker_sources_path:
        #Create an empty marker list file
        with open(f'{temp_folder}/{marker_list_file}', 'w') as f:
            pass
        logging.info(f"Created empty marker list file: {temp_folder}/{marker_list_file}")

        for file in marker_sources_path:
            logging.info(f'Adding marker from: {file}')
            append_file_content(file, f'{temp_folder}/{marker_list_file}')

        #Check if marker list file is not empty
        if os.path.getsize(f'{temp_folder}/{marker_list_file}') > 0:
            #Append the markers to the genome file
            logging.info(f"Adding collated markers to: {temp_folder}/{genome_with_markers}")
            append_file_content(f'{temp_folder}/{marker_list_file}', f'{temp_folder}/{genome_with_markers}')
                
        else:
            logging.warning("Marker list file is empty. No markers added to the genome.")
    else:
        logging.info("No marker sequences specified. Using reference genome without modifications.")
    
    #Verify the final genome file
    if check_file_exists(f'{temp_folder}/{genome_with_markers}'):
        logging.info(f"Final genome file size: {os.path.getsize(f'{temp_folder}/{genome_with_markers}')} bytes")

    #Builds the indices for the genome that has now been augmented with the transgenes, necessary for alignment
    logging.info(f"Building BWA index for: {temp_folder}/{genome_with_markers}")
    run_command(f'bwa index -b {base_block} {temp_folder}/{genome_with_markers}')
    run_command(f'cp {temp_folder}/{genome_with_markers} {folder_insertion}/{genome_with_markers}')

    if args.keepgoing:
        logging.info("Finished beginning, continuing to align")
        args.start = "align"

if args.start == "align" or args.start == "all":
    print("Starting alignment process")
    logging.info("Starting alignment process")

    #Aligns the new genome using BWA-MEM
    print("Aligning reads to genome using BWA-MEM")
    logging.info(f"Aligning reads to genome using BWA-MEM with {threads} threads")
    alignment_command = f'bwa mem -M -t {threads} {temp_folder}/{genome_with_markers} {reads_1_path} {reads_2_path} > {temp_folder}/{aligned_with_markers}'
    run_command(alignment_command)

    #Check if output file was created
    check_mandatory_files(f"{temp_folder}/{aligned_with_markers}")
    logging.info("Alignment completed successfully")

    if args.keepgoing:
        logging.info("Finished align, continuing to markers")
        args.start = "markers"
    
if args.start == "markers" or args.start == "all":
    print("Starting markers processing")
    logging.info("Starting markers processing")

    run_command(f'mkdir -p {folder_insertion}')

    if not marker_sources_path:
        logging.info("No marker sequences specified. Skipping marker detection.")
    else:
        #Check if marker list file exists
        check_mandatory_files(f'{temp_folder}/{marker_list_file}')
        
        # Make an empty list for the aligned diff chr
        markers = []
        with open(f'{temp_folder}/{marker_list_file}', 'r') as f:
            for line in f:
                if line.startswith('>'):
                    markers.append(line.strip()[1:])  # '>' is removed from the marker name
        
        print(f"Markers: {markers}")            
        logging.info(f"Markers: {markers}")

        #Add mitochondrial marker if specified
        if args.mitochondrial:
            markers += "|chrM"
            logging.info("Added mitochondrial marker")
        
        #Use functions based on pysam to select reads with chimeric characteristics
        for marker in markers:
            try:
                print(f"Processing marker: {marker}")
                logging.info(f"Processing marker: {marker}")
                logging.info(f"Processing file: {aligned_with_markers}")
                #parameters: input_file, output_file, markers, num_threads, max_mem_per_process
                #process_file is made upon the pysam library
                total_counts = process_file(
                    input_file=f"{temp_folder}/{aligned_with_markers}",
                    output_file=f"{temp_folder}/{aligned_diff_chr}",
                    reference_file=f"{temp_folder}/{genome_with_markers}",
                    markers=markers,
                    threads=threads,
                    max_mem=max_mem
                )

            except Exception as e:
                logging.error(f"Failed to process alignments for marker {marker}: {str(e)}")
                sys.exit(2)

        #Check if output file was created
        if os.path.exists(f"{temp_folder}/{aligned_diff_chr}"):
            file_size = os.path.getsize(f"{temp_folder}/{aligned_diff_chr}")
            logging.info(f"Size of {aligned_diff_chr}: {file_size} bytes")
            if file_size == 0:
                logging.warning(f"{aligned_diff_chr} is empty. This might indicate an issue with the filtering process.")
        else:
            logging.error(f"{aligned_diff_chr} was not created")
            sys.exit(2)    

    #Sort and index BAM file
    #This process is necessary nevertheless with and without markers flag
    print("Sorting and indexing BAM file")
    logging.info("Sorting and indexing BAM file")
    run_command(f"samtools sort -@ {threads} {temp_folder}/{aligned_with_markers} > {folder_insertion}/{aligned_diff_chr_bam}")
    run_command(f"samtools index -@ {threads} {folder_insertion}/{aligned_diff_chr_bam}")

    #Create header from BAM file
    print("Creating header from BAM file")
    logging.info("Creating header from BAM file")
    run_command(f'samtools view -@ {threads} -H {folder_insertion}/{aligned_diff_chr_bam} > {temp_folder}/{header}')

    print("Markers processing completed")
    logging.info("Marker processing completed")

    if args.keepgoing:
        logging.info("Finished markers, continuing to readout")
        print("Finished markers, continuing to readout")
        args.start = "readout"
        
if args.start == "readout" or args.start == "all":
    print("Starting readout processing")
    logging.info("Starting readout processing")
    
    #Check if either untargeted or targeted is specified
    if not (args.untargeted or args.targeted):
        logging.warning("Neither untargeted nor targeted markers specified. Skipping readout processing.")
        print("Neither untargeted nor targeted markers specified. Skipping readout processing.")
    else:
        #Check if marker list file exists
        check_file_exists(f'{temp_folder}/{marker_list_file}')

        #Read marker list
        with open(f'{temp_folder}/{marker_list_file}') as f:
            marker_list = [i[1:].strip() for i in f if i.startswith('>')]
        logging.info(f"Processed {len(marker_list)} markers from marker list file")
        logging.info(f"Markers: {marker_list}")

        #Check if aligned diff chr file exists
        check_file_exists(f'{temp_folder}/{aligned_diff_chr}')

        #Process data
        logging.info(f"Grouping alignment data from file: {temp_folder}/{aligned_diff_chr}'")
        data = group(f'{temp_folder}/{aligned_diff_chr}')
        
        if data is None:
            logging.warning("No data found in aligned diff chr file. This might indicate an issue with the alignment process or file content.")
            
        else:
            logging.info(f"Successfully grouped alignment data. Number of reference chromosomes: {len(data)}")
            for ref_chrom, alignments in data.items():
                logging.info(f"Reference chromosome {ref_chrom} has alignments to {len(alignments)} chromosomes")

            insertions = compress(data, granularity_threshold)
            if insertions is None:
                logging.info("No insertions found in data")

            else:
                logging.info(f"Compressed insertions: {len(insertions)}")
                logging.info("Filtering and scoring insertions")
                filteredInsertions = filterAndScore(temp_folder, folder_insertion, aligned_diff_chr_bam, insertions, args.spurious_filtering_threshold, args.stringency)
                if filteredInsertions is None:
                    logging.info("No insertions passed filtering and scoring")

                logging.info("Generating readout")
                #Note: filteredInsertions[0] (original_dict) is not used in the current implementation of readout function
                success = readout(folder_insertion, filteredInsertions[1], marker_list, min_matches)
                if not success:
                    logging.error("Failed to generate readout")
                    print("Error: Failed to generate readout. Check the log for details.")
                    sys.exit(2)
            logging.info("Readout processing completed")

        if args.keepgoing:
            logging.info("Finished readout, continuing to CNV")
            args.start = "cnv"

if args.start == "cnv" or args.start == "all":
    print("Starting CNV analysis")
    logging.info("Starting CNV analysis")

    run_command(f'mkdir -p {folder_cnv}')

    #Check if BAM file exists
    check_mandatory_files(f'{folder_insertion}/{aligned_diff_chr_bam}')
    cnvpytor_commands = []
    
    if exact_path is None:
        logging.info("Using default genome for CNV analysis")
        cnvpytor_commands = [
            f'cnvpytor -root {folder_cnv}/{pytor} -rd {folder_insertion}/{aligned_diff_chr_bam}',
            f'cnvpytor -root {folder_cnv}/{pytor} -his {bin_size}',
            f'cnvpytor -root {folder_cnv}/{pytor} -partition {bin_size}',
            f'cnvpytor -root {folder_cnv}/{pytor} -call {bin_size} > {folder_cnv}/calls.{bin_size}.tsv',
            f'cnvpytor -root {folder_cnv}/{pytor} -plot manhattan {bin_size} -o {folder_cnv}/{output_name}.png'
        ]
    else:
        logging.info("Configuring genome for CNV analysis")
        if not genome_configurator(temp_folder, pytor_conf, pytor_gc, genome_with_markers, header):
            logging.error("Failed to configure genome for CNV analysis: str{e}")
            sys.exit(2)
        
        cnvpytor_commands = [
            f'cnvpytor -conf {temp_folder}/{pytor_conf} -root {folder_cnv}/{pytor} -rd {folder_insertion}/{aligned_diff_chr_bam}',
            f'cnvpytor -conf {temp_folder}/{pytor_conf} -root {folder_cnv}/{pytor} -his {bin_size}',
            f'cnvpytor -conf {temp_folder}/{pytor_conf} -root {folder_cnv}/{pytor} -partition {bin_size}',
            f'cnvpytor -conf {temp_folder}/{pytor_conf} -root {folder_cnv}/{pytor} -call {bin_size} > {folder_cnv}/calls.{bin_size}.tsv',
            f'cnvpytor -conf {temp_folder}/{pytor_conf} -root {folder_cnv}/{pytor} -plot manhattan {bin_size} -o {folder_cnv}/{output_name}.png'
        ]

    for i, cmd in enumerate(cnvpytor_commands, 1):
        logging.info(f"Running CNVpytor step {i}/{len(cnvpytor_commands)}")
        run_command(cmd)

    print("CNV analysis completed")
    logging.info("CNV analysis completed")

    if args.keepgoing:
        logging.info("Finished CNVpytor, continuing to plots")
        args.start = "plots"

if args.start == "plots" or args.start == "all":
    print("Starting plots generation")
    logging.info("Starting plots generation")

    run_command(f'mkdir -p {folder_cnv}')
        
    #commands re-generation if not already defined
    if 'exact_path' in locals() and exact_path is not None:
        try:
            logging.info("Regenerating commands in commandHandler")
            commands = commandHandler(genome_source_path, exact_path, temp_folder, genome_with_markers, return_commands_only=True)
        except Exception as cmd_error:
            logging.error(f"Error in commandHandler: {cmd_error}")
            commands = []
    else:
        logging.info("exact_path is not defined or None. Setting commands to empty list.")
        commands = []

    #header re-generation if not already defined
    header_file_name = f'{temp_folder}/temp_header.sam'
    if 'header' not in locals() or header is None:
        logging.warning("Warning: 'header' is not defined. Attempting to recreate it.")
        logging.info("Retrieving header from BAM file")
        run_command(f'samtools view -H {folder_insertion}/{aligned_diff_chr_bam} > {header_file_name}')
            
        #Check if header file was created
        with open(header_file_name, 'r') as f:
             header_content = f.read()
        logging.info(f"Header content (first 200 characters): {header_content[:200]}")
            
        header = header_file_name  #set header to the new file name

    #bed_file execution
    try:
        print("Executing region_bed function")
        logging.info("Executing region_bed function")
        logging.info(f"temp_folder: {temp_folder}")
        logging.info(f"header file: {header}")
        logging.info(f"commands: {commands}")
        logging.info(f"marker_list: {marker_list}")
        logging.info(f"bed_file: {bed_file}")
        
        if all(var is not None for var in [temp_folder, header, commands, marker_list, bed_file]):
            region_bed(temp_folder, header, commands, marker_list, bed_file)
            logging.info(f"Bed file created successfully: {bed_file}")
        else:
            missing_vars = [var for var in ['temp_folder', 'header', 'commands', 'marker_list', 'bed_file'] if locals().get(var) is None]
            logging.error(f"Error: Missing required variables for region_bed: {', '.join(missing_vars)}")
            
    except Exception as region_bed_error:
        print(f"Error in creating bed file: {region_bed_error}")
        logging.error(f"Error in region_bed: {region_bed_error}")
        logging.warning("Failed to create bed file. This may affect subsequent steps.")

    #Check if bed file was created
    if check_file_exists(f"{temp_folder}/{bed_file}"):
        logging.info(f"Bed file exists at: {temp_folder}/{bed_file}")
    else:
        logging.warning(f"Warning: Bed file not found at: {temp_folder}/{bed_file}")
        logging.warning("This may affect IGV screenshot generation.")
        
    #Plotting Logic
    if not args.manual_plots:
        logging.info("Generating IGV plots")
        if args.gtf is not None:
            gtf = new_gtf_sorted  #don't put {folder_insertion} at the front
        else:
            gtf = None
        
        try:
            logging.info(f"Attempting to generate IGV screenshots with parameters: temp_folder={temp_folder}, folder_insertion={folder_insertion}, bam={folder_insertion}/{aligned_diff_chr_bam}, genome={folder_insertion}/{genome_with_markers}, bed_file={bed_file}, gtf_file={gtf}")
            result = igvScreenshot_new(temp_folder, folder_insertion, f'{folder_insertion}/{aligned_diff_chr_bam}', f'{folder_insertion}/{genome_with_markers}', bed_file, gtf_file=gtf)
            if not result:
                raise Exception("IGV screenshot generation returned False")
            logging.info("Successfully generated IGV screenshots")
        except Exception as e:
            logging.error(f"Failed to generate IGV screenshots: {str(e)}")
            print(f"Error: Failed to generate IGV screenshots: {str(e)}")
            
    else:
        logging.info("Generating depth statistics")
        run_command(f'samtools depth -b {bed_file} {folder_insertion}/{aligned_diff_chr_bam} > {temp_folder}/{overall_coverage}')
        
        logging.info("Generating histograms of depth")
        if not chrHistograms(f'{temp_folder}/{overall_coverage}', marker_list):
            logging.error("Failed to generate histograms")
            sys.exit(2)
        
        run_command(f'mv fig_* {folder_insertion}')
        run_command(f'mv *.cov {temp_folder}')
        
    logging.info("Plot generation completed")

    if args.keepgoing:
        print("Finished plots, continuing to Kraken (if enabled)")
        logging.info("Finished plots, continuing to Kraken (if enabled)")
        args.start = "kraken"

if args.start == "kraken" or args.start == "all":
    print("Starting Kraken/Bracken analysis")
    logging.info("Starting Kraken/Bracken analysis")

    run_command(f'mkdir -p {folder_kraken}')

    if args.kraken:
        logging.info("Kraken analysis enabled")

        #Check if input BAM file exists
        check_mandatory_files(f'{folder_insertion}/{aligned_diff_chr_bam}')

        logging.info("Extracting unaligned reads")
        run_command(f'samtools view -f 13 -@ {threads} {folder_insertion}/{aligned_diff_chr_bam} > {temp_folder}/{unaligned}')

        logging.info("Separating unaligned reads into forward and reverse")
        run_command(f'samtools fastq -@ {threads} {temp_folder}/{unaligned} -1 {temp_folder}/{unaligned_R1} -2 {temp_folder}/{unaligned_R2}')

        logging.info("Running Kraken2")
        kraken_cmd = f'kraken2 --threads {threads} --db {database} --report {folder_kraken}/classified_seqs_{output_name}.kreport --paired --classified-out {folder_kraken}/classified_seqs_{output_name}#.fq {temp_folder}/{unaligned_R1} {temp_folder}/{unaligned_R2} > {folder_kraken}/classified_{output_name}.kraken'
        run_command(kraken_cmd)

        #Check if Kraken2 output file exists
        check_mandatory_files(f'{folder_kraken}/classified_seqs_{output_name}.kreport')

        logging.info("Running Bracken")
        bracken_cmd = f'bracken -d {database} -i {folder_kraken}/classified_seqs_{output_name}.kreport -o {folder_kraken}/classified_seqs_{output_name}.bracken -r 150'
        run_command(bracken_cmd)

        #Check if Bracken output file exists
        check_mandatory_files(f'{folder_kraken}/classified_seqs_{output_name}.bracken')

        logging.info("Kraken/Bracken analysis completed successfully")
    else:
        logging.info("Kraken analysis not enabled, skipping")

    if args.keepgoing:
        print("Finished Kraken, continuing to variant calling (if enabled)")
        logging.info("Finished Kraken, continuing to variant calling (if enabled)")
        args.start = "variant"

if args.start == "variant" or args.start == "all":
    print("Starting variant calling")
    logging.info("Starting variant calling")

    run_command(f'mkdir -p {folder_snp}')

    if args.variant_calling is not None and args.variant_calling != []:
        logging.info("Variant calling enabled")

        #Set up variables for variant calling
        variant_genome_source_path = pathFinder(args.variant_calling[0])
        clinvardb_source_path = pathFinder(args.variant_calling[1])

        #Check if input files exist
        check_mandatory_files(variant_genome_source_path)
        check_mandatory_files(clinvardb_source_path)

        #Check and create BWA index
        bwa_index_files = [".bwt", ".pac", ".sa", ".ann", ".amb"]
        if all(os.path.isfile(variant_genome_source_path + ext) for ext in bwa_index_files):
            logging.info("BWA index files detected. Skipping BWA indexing.")
        else:
            logging.info("Creating BWA index")
            run_command(f"bwa index {variant_genome_source_path}")

        #Check and create samtools faidx
        if os.path.isfile(variant_genome_source_path + ".fai"):
            logging.info("Samtools faidx index detected. Skipping samtools faidx.")
        else:
            logging.info("Creating samtools faidx index")
            run_command(f"samtools faidx {variant_genome_source_path}")

        #Realign reads to the new genome
        print("Realigning reads to GRCh38")
        logging.info("Realigning reads to GRCh38")
        bwa_cmd = f"bwa mem -M -t {threads} {variant_genome_source_path} {reads_1_path} {reads_2_path} > {temp_folder}/{variant_genome_aligned}"
        run_command(bwa_cmd)

        #Sort and index the aligned BAM file
        print("Sorting and indexing the aligned BAM file")
        logging.info("Sorting and indexing the aligned BAM file")
        run_command(f"samtools sort -@ {threads} {temp_folder}/{variant_genome_aligned} > {temp_folder}/{variant_genome_aligned_bam}")
        run_command(f"samtools index -@ {threads} {temp_folder}/{variant_genome_aligned_bam}")

        #Generate VCF using bcftools
        print("Generating VCF using bcftools")
        logging.info("Generating VCF using bcftools")
        bcftools_cmd = f"bcftools mpileup -Ou -f {variant_genome_source_path} {temp_folder}/{variant_genome_aligned_bam} | bcftools call -mv -Ov -o {temp_folder}/{variant_vcf}"
        run_command(bcftools_cmd)

        #Check if VCF file was created
        check_mandatory_files(f"{temp_folder}/{variant_vcf}")

        print("Variant calling completed successfully")
        logging.info("Variant calling completed successfully")

        if args.keepgoing:
            logging.info("Finished variant calling, continuing to SNP filtering")
            args.start = "snp_filtering"
    else:
        logging.info("Variant calling not enabled, skipping")
        
if args.start == "snp_filtering" or args.start == "all":
    print("Starting SNP filtering")
    logging.info("Starting SNP filtering")

    if args.variant_calling is not None and args.variant_calling != []:
        variant_genome_source_path = pathFinder(args.variant_calling[0])
        clinvardb_source_path = pathFinder(args.variant_calling[1])

        #Check if input files exist
        check_mandatory_files(f"{temp_folder}/{variant_vcf}")
        check_mandatory_files(clinvardb_source_path)

        #Run snpEff
        print("Running snpEff for variant annotation")
        logging.info("Running snpEff for variant annotation")
        snpeff_cmd = f"snpEff -dataDir seqverify_defaults -v GRCh38 -stats {temp_folder}/snpeff_summary.html -csvStats {temp_folder}/snpeff_summary.csv {temp_folder}/{variant_vcf} > {temp_folder}/{variant_vcf_ann}"
        run_command(snpeff_cmd)

        #Check if snpEff output file was created
        check_mandatory_files(f"{temp_folder}/{variant_vcf_ann}")

        #Run SnpSift
        logging.info("Running SnpSift for ClinVar annotation")
        snpsift_cmd = f"SnpSift annotate -v {clinvardb_source_path} {temp_folder}/{variant_vcf_ann} > {folder_snp}/{variant_vcf_ann}"
        run_command(snpsift_cmd)

        #Check if SnpSift output file was created
        check_mandatory_files(f"{folder_snp}/{variant_vcf_ann}")

        #Run mutation logger
        logging.info("Running mutation logger for final filtering")
        try:
            if exact_path is None:
                mutation_logger(folder_snp, variant_lof, variant_vcf_ann, args.min_quality, args.variant_intensity, args.variant_window_size)
            else:
                mutation_logger(folder_snp, variant_lof, variant_vcf_ann, args.min_quality, args.variant_intensity, args.variant_window_size, exact_path)
        except Exception as e:
            logging.error(f"Mutation logger failed: {e}")
            sys.exit(2)

        #Check if mutation logger output file was created
        check_mandatory_files(f"{folder_snp}/{variant_lof}")

        print("SNP filtering completed successfully")
        logging.info("SNP filtering completed successfully")
    else:
        logging.info("Variant calling was not enabled, skipping SNP filtering")

#Temporary folder cleanup
if args.keep_temp:
    logging.info(f"Keeping temporary folder: {temp_folder}")
else:
    logging.info(f"Attempting to remove temporary folder: {temp_folder}")
    try:
        if os.path.exists(temp_folder):
            shutil.rmtree(temp_folder)
            logging.info(f"Successfully removed temporary folder: {temp_folder}")
        else:
            logging.info(f"Temporary folder not found: {temp_folder}")
    except Exception as e:
        print(f"Warning: Failed to remove temporary folder: {e}")
        logging.info(f"You may need to manually remove the temporary folder: {temp_folder}")

print("SeqVerify pipeline completed")
logging.info("SeqVerify pipeline completed")